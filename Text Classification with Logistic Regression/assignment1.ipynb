{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Text Classification with Logistic Regression\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop and test two text classification systems: \n",
    "\n",
    "- **Task 1:** sentiment analysis, in particular to predict the sentiment of movie review, i.e. positive or negative (binary classification).\n",
    "- **Task 2:** topic classification, to predict whether a news article is about International issues, Sports or Business (multiclass classification).\n",
    "\n",
    "\n",
    "### Data - Task 1 \n",
    "\n",
    "The data you will use for Task 1 are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "### Data - Task 2\n",
    "\n",
    "The data you will use for Task 2 is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:31:36.292691Z",
     "start_time": "2020-02-15T14:31:35.549108Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.145788Z",
     "start_time": "2020-02-15T14:17:28.066100Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the training, development and test sets\n",
    "train_data = pd.read_csv('data_sentiment/train.csv',header=None, names=['text','label'])\n",
    "dev_data = pd.read_csv('data_sentiment/dev.csv',header=None, names=['text','label'])\n",
    "test_data = pd.read_csv('data_sentiment/test.csv',header=None, names=['text','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every once in a while you see a film that is s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i was growing up in 1970s , boys in my sc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the muppet movie is the first , and the best m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  note : some may consider portions of the follo...      1\n",
       "1  note : some may consider portions of the follo...      1\n",
       "2  every once in a while you see a film that is s...      1\n",
       "3  when i was growing up in 1970s , boys in my sc...      1\n",
       "4  the muppet movie is the first , and the best m...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.115577Z",
     "start_time": "2020-02-15T14:17:31.108038Z"
    }
   },
   "outputs": [],
   "source": [
    "# put the raw texts into Python lists\n",
    "train_text = train_data['text'].tolist()\n",
    "dev_text = dev_data['text'].tolist()\n",
    "test_text = test_data['text'].tolist()\n",
    "\n",
    "# put their corresponding labels into NumPy arrays\n",
    "train_label = np.array(train_data['label'])\n",
    "dev_label = np.array(dev_data['label'])\n",
    "test_label = np.array(test_data['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Representation \n",
    "\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should: \n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- compute bigrams, trigrams given the remaining unigrams\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (you can keep top N if you encounter memory issues).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.860420Z",
     "start_time": "2020-02-15T14:17:31.855439Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they' 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `x` : a list of all extracted features.(Remove the same ngram)\n",
    "- `text_ngrams` : a list of all extracted features.(split for several list by different rows of `x_raw`（Two-dimensional）)\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:33.169090Z",
     "start_time": "2020-02-15T14:17:33.161268Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', stop_words=[], vocab=set()):    \n",
    "    #create list x\n",
    "    x = []\n",
    "    #create list text_ngrams(store all extracted ngrams of documents)\n",
    "    text_ngrams = []\n",
    "    #extract texts according to different documents\n",
    "    for raw in x_raw:\n",
    "        #create list n_g_list(store a document of the extracted ngrams)  \n",
    "        n_g_list = []\n",
    "        # use stop_words to delete these words in document\n",
    "        train_token = re.findall(token_pattern,raw)\n",
    "        train_stop = []\n",
    "        for raw_token in train_token:\n",
    "            if raw_token not in stop_words:\n",
    "                train_stop.append(raw_token)\n",
    "        #use n_g_list list to save the extracted ngrams of a document.\n",
    "        for i in range(min(ngram_range),max(ngram_range)+1):\n",
    "            length = len(train_stop)-i+1\n",
    "            for j in range(length):\n",
    "                if i ==1:\n",
    "                    unigram = train_stop[j]\n",
    "                    x.append(unigram)\n",
    "                    n_g_list.append(unigram)\n",
    "                else:\n",
    "                    ngrams = tuple(train_stop[j:j+i])\n",
    "                    x.append(ngrams)\n",
    "                    n_g_list.append(ngrams)\n",
    "        text_ngrams.append(n_g_list)\n",
    "    x = set(x)\n",
    "    \n",
    "    return x, text_ngrams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all extracted features for train set\n",
    "tr_x, tr_text_ngrams = extract_ngrams(x_raw = train_text, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=5000, stop_words=[]):\n",
    "    \n",
    "    x, ngrams_list = extract_ngrams(x_raw=X_raw, stop_words=stop_words)\n",
    "    \n",
    "    df = {}    \n",
    "    # use dict(df) to save all ngrams as keys and their corresponding document frequency as values               \n",
    "    for grams in ngrams_list:\n",
    "        grams_uni = set(grams)\n",
    "        for n_gram in grams_uni:\n",
    "            if n_gram not in df.keys():\n",
    "                df[n_gram]=1\n",
    "            else:\n",
    "                df[n_gram]+=1\n",
    "\n",
    "    #order by the dict.values              \n",
    "    df = dict(sorted(df.items(), key=lambda x:x[1], reverse=True))\n",
    "    #keep top-N more frequent ngrams\n",
    "    df = {k: df[k] for k in list(df.keys())[:keep_topN]}\n",
    " \n",
    "    vocab= [ i for i in df.keys()]\n",
    "    ngram_counts= [ i for i in df.values()]\n",
    "    \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#according to train set to got my vocabulary as feature\n",
    "vocab, tr_df, tr_ngram_counts = get_vocab(X_raw=train_text, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and id -> word dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.326811Z",
     "start_time": "2020-02-15T14:17:39.322256Z"
    }
   },
   "outputs": [],
   "source": [
    "# create word dictionaries \n",
    "vocab_dict = {}\n",
    "for i in range(len(vocab)):\n",
    "    vocab_dict[i] = vocab[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_x, dev_text_ngrams = extract_ngrams(x_raw = dev_text, stop_words=stop_words)\n",
    "test_x, test_text_ngrams = extract_ngrams(x_raw = test_text, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.219201Z",
     "start_time": "2020-02-15T14:17:40.215129Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorise(X_ngram, vocab):\n",
    "    #Create a two-dimensional array（initialize all zero）, the number of rows is the number of documents, the number of columns is the number of features\n",
    "    X_vec = np.zeros((len(X_ngram),len(vocab)))\n",
    "    for i, text in enumerate(X_ngram):\n",
    "        for j, word in enumerate(vocab):\n",
    "            #count the number of words of vocab appearing in the document.\n",
    "            count = text.count(word)\n",
    "            X_vec[i][j]=count\n",
    "                \n",
    "    \n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain document vectors for each document in the train, development and test set\n",
    "X_tr_count = vectorise(X_ngram=tr_text_ngrams, vocab=vocab)\n",
    "X_dev_count = vectorise(X_ngram=dev_text_ngrams, vocab=vocab)\n",
    "X_test_count = vectorise(X_ngram=test_text_ngrams, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  8., 20., ...,  0.,  1.,  1.],\n",
       "       [ 2.,  5.,  6., ...,  0.,  0.,  0.],\n",
       "       [ 5.,  4.,  8., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 3.,  2.,  4., ...,  0.,  0.,  0.],\n",
       "       [11.,  7., 11., ...,  0.,  0.,  0.],\n",
       "       [ 2.,  3.,  8., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.022692Z",
     "start_time": "2020-02-15T14:17:42.012315Z"
    }
   },
   "outputs": [],
   "source": [
    "# create idf vocabulary dictiongnary for training data\n",
    "vocab_idfs_tr={}\n",
    "for word in vocab:\n",
    "    doc_count=0\n",
    "    for text in tr_text_ngrams:\n",
    "        if word in text:\n",
    "            doc_count+=1\n",
    "    vocab_idfs_tr[word]=np.log((len(tr_text_ngrams)/doc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create idf vocabulary dictiongnary for development data\n",
    "vocab_idfs_dev={}\n",
    "for word in vocab:\n",
    "    doc_count=0\n",
    "    for text in dev_text_ngrams:\n",
    "        if word in text:\n",
    "            doc_count+=1\n",
    "    vocab_idfs_dev[word]=np.log((len(dev_text_ngrams)+1) / (doc_count+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create idf vocabulary dictiongnary for test data\n",
    "vocab_idfs_test={}\n",
    "for word in vocab:\n",
    "    doc_count=0\n",
    "    for text in test_text_ngrams:\n",
    "        if word in text:\n",
    "            doc_count+=1\n",
    "    vocab_idfs_test[word]=np.log((len(test_text_ngrams)+1) / (doc_count+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training set, extract tf.idf vectors\n",
    "tfidf_mat=[]\n",
    "for i, word in enumerate(vocab_idfs_tr.keys()):\n",
    "    # get the i column feature value\n",
    "    value = X_tr_count[:,i]\n",
    "    # tf * idf\n",
    "    tfidf = value * vocab_idfs_tr[word]\n",
    "    tfidf_mat.append(tfidf)\n",
    "\n",
    "X_tr_tfidf = np.asarray(tfidf_mat)\n",
    "X_tr_tfidf = np.transpose(X_tr_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development set, extract tf.idf vectors\n",
    "tfidf_mat=[]\n",
    "for i, word in enumerate(vocab_idfs_dev.keys()):\n",
    "    value = X_dev_count[:,i]\n",
    "    tfidf = value * vocab_idfs_dev[word]\n",
    "    tfidf_mat.append(tfidf)\n",
    "\n",
    "X_dev_tfidf = np.asarray(tfidf_mat)\n",
    "X_dev_tfidf = np.transpose(X_dev_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test set, extract tf.idf vectors\n",
    "tfidf_mat=[]\n",
    "for i, word in enumerate(vocab_idfs_test.keys()):\n",
    "    value = X_test_count[:,i]\n",
    "    tfidf = value * vocab_idfs_test[word]\n",
    "    tfidf_mat.append(tfidf)\n",
    "\n",
    "X_test_tfidf = np.asarray(tfidf_mat)\n",
    "X_test_tfidf = np.transpose(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 5000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28974173, 0.92585256, 2.57290779, ..., 0.        , 4.41101417,\n",
       "        4.41101417],\n",
       "       [0.09658058, 0.57865785, 0.77187234, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.24145145, 0.46292628, 1.02916312, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.14487087, 0.23146314, 0.51458156, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.53119318, 0.81012099, 1.41509928, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.09658058, 0.34719471, 1.02916312, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:43.211619Z",
     "start_time": "2020-02-15T14:17:43.207266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09658058, 0.57865785, 0.77187234, 0.35893698, 0.90330287,\n",
       "       0.        , 0.51902239, 0.87162991, 0.58299774, 0.88311318,\n",
       "       0.89175457, 1.26667844, 0.65304381, 0.        , 0.74419883,\n",
       "       0.79882407, 0.        , 0.        , 0.83970769, 1.28576258,\n",
       "       0.        , 0.        , 0.94229428, 0.99563077, 0.50489089,\n",
       "       0.52884413, 0.55711891, 2.86730597, 0.590075  , 0.59394847,\n",
       "       0.60043778, 1.20348143, 0.        , 2.45417652, 0.61750977,\n",
       "       0.62149119, 0.        , 0.        , 3.16765735, 0.68178342,\n",
       "       1.3663937 , 0.        , 0.        , 0.70608769, 0.        ,\n",
       "       2.17524068, 0.74444047, 0.74594537, 1.51004517, 0.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tfidf[1,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.160661Z",
     "start_time": "2020-02-15T14:17:44.157902Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    sig = 1. / (1 + np.exp(-z))\n",
    "    \n",
    "    return sig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.00669285 0.76852478]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(0)) \n",
    "print(sigmoid(np.array([-5., 1.2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.718566Z",
     "start_time": "2020-02-15T14:17:44.715017Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    dot_p = np.dot(X,weights)\n",
    "    preds_proba = sigmoid(dot_p)\n",
    "    \n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.002125Z",
     "start_time": "2020-02-15T14:17:44.998668Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    proba = predict_proba(X = X, weights = weights)\n",
    "    # if the probability of the class less than 0.5, then the label for this class is 0, otherwise, 1.\n",
    "    proba[proba<0.5]=0\n",
    "    proba[proba>=0.5]=1\n",
    "    preds_class = proba.astype(np.int64)\n",
    "       \n",
    "    return preds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.455533Z",
     "start_time": "2020-02-15T14:17:45.451475Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_loss(X, Y, weights, alpha=0.00001):\n",
    "    \n",
    "    \n",
    "    y_pred = predict_proba(X = X, weights = weights)\n",
    "    # Clip y_pred between alpha and 1-alpha\n",
    "    y_pred = np.clip(y_pred, alpha, 1-alpha)\n",
    "    loss = - Y * np.log(y_pred) - (1 - Y) * np.log(1-y_pred)\n",
    "    L2_regularization = (alpha/2)*(np.sum(np.square(weights)))\n",
    "    # Use L2 regularisation\n",
    "    l = loss + L2_regularization\n",
    "\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.968510Z",
     "start_time": "2020-02-15T14:17:45.958185Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], lr=0.1, alpha=0.00001, epochs=100, tolerance=0.0001, print_progress=True):\n",
    "    \n",
    "    pre_loss_dev = 10.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    weights = np.zeros(X_tr.shape[1])\n",
    "    \n",
    "    #Perform multiple epochs over the training data \n",
    "    for i in range(epochs):\n",
    "        #Randomise the order of training data after each epoch\n",
    "        np.random.seed(i)\n",
    "        new_X_tr = np.random.permutation(X_tr)\n",
    "        np.random.seed(i)\n",
    "        new_Y_tr = np.random.permutation(Y_tr)\n",
    "        # The list store training loss for each document.\n",
    "        tr_loss_list = []\n",
    "        for j, X_tr_row in enumerate(new_X_tr):\n",
    "            tr_loss = binary_loss(X = X_tr_row, Y=new_Y_tr[j], weights = weights, alpha=alpha)\n",
    "            tr_loss_list.append(tr_loss)\n",
    "            y_pred = predict_proba(X = X_tr_row, weights = weights)\n",
    "            error = y_pred-new_Y_tr[j]\n",
    "            weights = weights - lr*error*X_tr_row\n",
    "        #get average training loss\n",
    "        aver_tr_loss = np.mean(tr_loss_list)\n",
    "        training_loss_history.append(aver_tr_loss)\n",
    "        #compute validation set loss\n",
    "        valid_loss = binary_loss(X = X_dev, Y=Y_dev, weights = weights, alpha=alpha)\n",
    "        valid_loss = sum(valid_loss)/len(valid_loss)\n",
    "        validation_loss_history.append(valid_loss)\n",
    "        \n",
    "        #After each epoch print the training and development loss\n",
    "        print('Epoch: %d' % i, '| Training loss: %f' % aver_tr_loss, '| Validation loss: %f' % valid_loss)\n",
    "        \n",
    "        #Stop training if the difference between the current and previous validation loss is smaller than tolerance\n",
    "        if (pre_loss_dev-valid_loss)<tolerance:\n",
    "            break\n",
    "        pre_loss_dev = valid_loss\n",
    "\n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with Count vectors\n",
    "\n",
    "First train the model using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.667093 | Validation loss: 0.648187\n",
      "Epoch: 1 | Training loss: 0.613701 | Validation loss: 0.620114\n",
      "Epoch: 2 | Training loss: 0.575545 | Validation loss: 0.597205\n",
      "Epoch: 3 | Training loss: 0.545176 | Validation loss: 0.575124\n",
      "Epoch: 4 | Training loss: 0.520335 | Validation loss: 0.562594\n",
      "Epoch: 5 | Training loss: 0.499663 | Validation loss: 0.549006\n",
      "Epoch: 6 | Training loss: 0.481563 | Validation loss: 0.537931\n",
      "Epoch: 7 | Training loss: 0.465454 | Validation loss: 0.529435\n",
      "Epoch: 8 | Training loss: 0.451275 | Validation loss: 0.520830\n",
      "Epoch: 9 | Training loss: 0.438504 | Validation loss: 0.513939\n",
      "Epoch: 10 | Training loss: 0.426575 | Validation loss: 0.505659\n",
      "Epoch: 11 | Training loss: 0.415875 | Validation loss: 0.501853\n",
      "Epoch: 12 | Training loss: 0.406412 | Validation loss: 0.494416\n",
      "Epoch: 13 | Training loss: 0.397229 | Validation loss: 0.489247\n",
      "Epoch: 14 | Training loss: 0.388496 | Validation loss: 0.483964\n",
      "Epoch: 15 | Training loss: 0.380787 | Validation loss: 0.479454\n",
      "Epoch: 16 | Training loss: 0.373154 | Validation loss: 0.475763\n",
      "Epoch: 17 | Training loss: 0.366089 | Validation loss: 0.472849\n",
      "Epoch: 18 | Training loss: 0.359430 | Validation loss: 0.468468\n",
      "Epoch: 19 | Training loss: 0.352456 | Validation loss: 0.464239\n",
      "Epoch: 20 | Training loss: 0.347019 | Validation loss: 0.461148\n",
      "Epoch: 21 | Training loss: 0.341035 | Validation loss: 0.459995\n",
      "Epoch: 22 | Training loss: 0.335681 | Validation loss: 0.455324\n",
      "Epoch: 23 | Training loss: 0.330425 | Validation loss: 0.452428\n",
      "Epoch: 24 | Training loss: 0.325092 | Validation loss: 0.450059\n",
      "Epoch: 25 | Training loss: 0.320788 | Validation loss: 0.447273\n",
      "Epoch: 26 | Training loss: 0.315995 | Validation loss: 0.445186\n",
      "Epoch: 27 | Training loss: 0.311371 | Validation loss: 0.444286\n",
      "Epoch: 28 | Training loss: 0.307147 | Validation loss: 0.440620\n",
      "Epoch: 29 | Training loss: 0.302942 | Validation loss: 0.438567\n",
      "Epoch: 30 | Training loss: 0.299131 | Validation loss: 0.436733\n",
      "Epoch: 31 | Training loss: 0.295140 | Validation loss: 0.435648\n",
      "Epoch: 32 | Training loss: 0.291410 | Validation loss: 0.433040\n",
      "Epoch: 33 | Training loss: 0.287561 | Validation loss: 0.431353\n",
      "Epoch: 34 | Training loss: 0.284058 | Validation loss: 0.429758\n",
      "Epoch: 35 | Training loss: 0.280808 | Validation loss: 0.429540\n",
      "Epoch: 36 | Training loss: 0.277696 | Validation loss: 0.427199\n",
      "Epoch: 37 | Training loss: 0.274513 | Validation loss: 0.425268\n",
      "Epoch: 38 | Training loss: 0.271223 | Validation loss: 0.423855\n",
      "Epoch: 39 | Training loss: 0.268385 | Validation loss: 0.423024\n",
      "Epoch: 40 | Training loss: 0.265460 | Validation loss: 0.421410\n",
      "Epoch: 41 | Training loss: 0.262470 | Validation loss: 0.420252\n",
      "Epoch: 42 | Training loss: 0.259608 | Validation loss: 0.419834\n",
      "Epoch: 43 | Training loss: 0.257117 | Validation loss: 0.417726\n",
      "Epoch: 44 | Training loss: 0.254452 | Validation loss: 0.416594\n",
      "Epoch: 45 | Training loss: 0.251744 | Validation loss: 0.415981\n",
      "Epoch: 46 | Training loss: 0.249335 | Validation loss: 0.414784\n",
      "Epoch: 47 | Training loss: 0.246855 | Validation loss: 0.414100\n",
      "Epoch: 48 | Training loss: 0.244396 | Validation loss: 0.412465\n",
      "Epoch: 49 | Training loss: 0.242162 | Validation loss: 0.411534\n",
      "Epoch: 50 | Training loss: 0.239812 | Validation loss: 0.410605\n",
      "Epoch: 51 | Training loss: 0.237590 | Validation loss: 0.410121\n",
      "Epoch: 52 | Training loss: 0.235449 | Validation loss: 0.409201\n",
      "Epoch: 53 | Training loss: 0.233311 | Validation loss: 0.408154\n",
      "Epoch: 54 | Training loss: 0.231189 | Validation loss: 0.407330\n",
      "Epoch: 55 | Training loss: 0.228959 | Validation loss: 0.407256\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr = X_tr_count, Y_tr = train_label, \n",
    "                                             X_dev=X_dev_count, \n",
    "                                             Y_dev=dev_label, \n",
    "                                             lr=0.0001,\n",
    "                                             alpha=0.0001, \n",
    "                                             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "The model is overfit, because the final loss function value of the training data is much lower than that of the validation data.\n",
    "The reason for this result is over-training the training set data, resulting in local optimization, and the model's generalization ability is not good, so the error rate of the verification set is higher than the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,len(loss_tr_count),len(loss_tr_count))\n",
    "y1, y2 = loss_tr_count, dev_loss_count\n",
    " \n",
    "plt.plot(x, y1,label='Training loss')\n",
    "plt.plot(x, y2, label='Validation loss')\n",
    " \n",
    "plt.title('Training Monitoring')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(Y_te,preds_te_count):\n",
    "    \n",
    "    count = 0\n",
    "    for i,label in enumerate(Y_te):\n",
    "        # when the prediction（preds_te_count） and the real label （Y_te）are the same\n",
    "        if preds_te_count[i] == label:\n",
    "            count = count+1\n",
    "            \n",
    "    return count/len(Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(Y_te,preds_te_count):\n",
    "    #The numerator is the number of the case where the prediction（preds_te_count） and the real label （Y_te）are both with 1.\n",
    "    num = ((Y_te==1)*(preds_te_count==1)).sum()\n",
    "    # The denominator is the number of the predic tion label is 1.\n",
    "    deno = (preds_te_count==1).sum()\n",
    "    return num/deno\n",
    "\n",
    "def recall_score(Y_te, preds_te_count):\n",
    "    num = ((Y_te==1)*(preds_te_count==1)).sum()\n",
    "    # The denominator is the number of the actual label is 1.\n",
    "    deno = (Y_te==1).sum()\n",
    "    return num/deno\n",
    "\n",
    "def f1_score(Y_te, preds_te_count):\n",
    "    num = 2 * precision_score(Y_te, preds_te_count) * recall_score(Y_te, preds_te_count)\n",
    "    deno = (precision_score(Y_te, preds_te_count) + recall_score(Y_te, preds_te_count))\n",
    "    return num/deno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:37:40.569499Z",
     "start_time": "2020-02-15T14:37:40.566796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8325\n",
      "Precision: 0.8275862068965517\n",
      "Recall: 0.84\n",
      "F1-Score: 0.8337468982630273\n"
     ]
    }
   ],
   "source": [
    "p_class_count = predict_class(X = X_test_count, weights = w_count)\n",
    "Y_te = test_label\n",
    "preds_te_count = p_class_count\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 positive words are ['great', 'well', 'also', 'life', 'seen', 'world', 'many', 'fun', 'movies', 'see'] respectively.\n",
      "The top 10 negative words are ['bad', 'only', 'worst', 'plot', 'unfortunately', 'script', 'why', 'any', 'boring', 'nothing'] respectively.\n"
     ]
    }
   ],
   "source": [
    "dic_count = {}\n",
    "for i,weight in enumerate(w_count):\n",
    "    dic_count[vocab[i]] = weight\n",
    "\n",
    "# get positive words\n",
    "dic_count_p = {k:v for k, v in dic_count.items() if v>=0}\n",
    "#order by weights\n",
    "dic_count_p= dict(sorted(dic_count_p.items(), key=lambda x:x[1], reverse=True))\n",
    "# select top 10 words\n",
    "dic_count_p = {k: dic_count_p[k] for k in list(dic_count_p.keys())[:10]}\n",
    "\n",
    "# get negative words\n",
    "dic_count_n = {k:v for k, v in dic_count.items() if v<0}\n",
    "#order by weights\n",
    "dic_count_n= dict(sorted(dic_count_n.items(), key=lambda x:x[1], reverse=False))\n",
    "# select top 10 words\n",
    "dic_count_n = {k: dic_count_n[k] for k in list(dic_count_n.keys())[:10]}\n",
    "\n",
    "print(\"The top 10 positive words are %s respectively.\"% list(dic_count_p.keys()))\n",
    "print(\"The top 10 negative words are %s respectively.\"% list(dic_count_n.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the obtained top10 words above have no meaning, such as also, many, any and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "We could use the classifier into a different domain such laptop reviews or restaurant reviews, because there are many adjectives and adverbs that can indicate positive or negative categories, such as 'great' and 'well' for positive features, 'bad', 'worst', 'unfortunately', etc. for negative features, these features would generalise well, so it's suitable in new fields. But there are still some of words doesn't make sense, such as  'also', 'many', etc., it's bug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lr = 0.001000, alpha = 0.010000;  F1-Score: 0.8286445012787724\n",
      "When lr = 0.001000, alpha = 0.001000;  F1-Score: 0.8286445012787724\n",
      "When lr = 0.001000, alpha = 0.000100;  F1-Score: 0.8286445012787724\n",
      "When lr = 0.000100, alpha = 0.010000;  F1-Score: 0.8320802005012531\n",
      "When lr = 0.000100, alpha = 0.001000;  F1-Score: 0.8337468982630273\n",
      "When lr = 0.000100, alpha = 0.000100;  F1-Score: 0.8337468982630273\n",
      "When lr = 0.000010, alpha = 0.010000;  F1-Score: 0.8148148148148149\n",
      "When lr = 0.000010, alpha = 0.001000;  F1-Score: 0.8148148148148149\n",
      "When lr = 0.000010, alpha = 0.000100;  F1-Score: 0.8148148148148149\n"
     ]
    }
   ],
   "source": [
    "# choose model hyperparameters: learning rate and regularisation strength\n",
    "lr_hyper = [0.001,0.0001,0.00001]\n",
    "alpha_hyper = [0.01,0.001,0.0001]\n",
    "\n",
    "for lr in range(len(lr_hyper)):\n",
    "    for alpha in range(len(alpha_hyper)):\n",
    "        w_count, loss_tr_count, dev_loss_count = SGD(X_tr = X_tr_count, Y_tr = train_label, \n",
    "                                             X_dev=X_dev_count, \n",
    "                                             Y_dev=dev_label, \n",
    "                                             lr=lr_hyper[lr],\n",
    "                                             alpha=alpha_hyper[alpha], \n",
    "                                             epochs=100)\n",
    "        preds = predict_class(X = X_test_count, weights = w_count)\n",
    "        # If test this, we need change `SGD` to comment out that print(add # in front of print, do not print loss), \n",
    "        #then we can just see the results of F1-Score for several hyperparameters.\n",
    "        print('When lr = %f, alpha = %f; '%(lr_hyper[lr],alpha_hyper[alpha]), 'F1-Score:', f1_score(Y_te = test_label, preds_te_count = preds))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that when lr = 0.0001 and alpha = 0.001(also 0.0001), the F1-Score value reaches the maximum, 0.8337. As lr continues to increase, the F1-Score value becomes smaller. \n",
    "\n",
    "In general, the learning rate(lr) has a greater impact on model training, while regularisation strength(alpha) has little effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with TF.IDF vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.630199 | Validation loss: 0.598519\n",
      "Epoch: 1 | Training loss: 0.492170 | Validation loss: 0.550431\n",
      "Epoch: 2 | Training loss: 0.415312 | Validation loss: 0.519785\n",
      "Epoch: 3 | Training loss: 0.363380 | Validation loss: 0.496656\n",
      "Epoch: 4 | Training loss: 0.325479 | Validation loss: 0.479463\n",
      "Epoch: 5 | Training loss: 0.295740 | Validation loss: 0.465533\n",
      "Epoch: 6 | Training loss: 0.271778 | Validation loss: 0.454433\n",
      "Epoch: 7 | Training loss: 0.251800 | Validation loss: 0.445154\n",
      "Epoch: 8 | Training loss: 0.235133 | Validation loss: 0.437123\n",
      "Epoch: 9 | Training loss: 0.220645 | Validation loss: 0.430326\n",
      "Epoch: 10 | Training loss: 0.207920 | Validation loss: 0.424697\n",
      "Epoch: 11 | Training loss: 0.196915 | Validation loss: 0.419430\n",
      "Epoch: 12 | Training loss: 0.187161 | Validation loss: 0.414936\n",
      "Epoch: 13 | Training loss: 0.178436 | Validation loss: 0.410911\n",
      "Epoch: 14 | Training loss: 0.170506 | Validation loss: 0.407503\n",
      "Epoch: 15 | Training loss: 0.163429 | Validation loss: 0.404300\n",
      "Epoch: 16 | Training loss: 0.156955 | Validation loss: 0.401359\n",
      "Epoch: 17 | Training loss: 0.150996 | Validation loss: 0.398844\n",
      "Epoch: 18 | Training loss: 0.145622 | Validation loss: 0.396521\n",
      "Epoch: 19 | Training loss: 0.140510 | Validation loss: 0.394573\n",
      "Epoch: 20 | Training loss: 0.136036 | Validation loss: 0.392635\n",
      "Epoch: 21 | Training loss: 0.131757 | Validation loss: 0.390830\n",
      "Epoch: 22 | Training loss: 0.127786 | Validation loss: 0.389305\n",
      "Epoch: 23 | Training loss: 0.124113 | Validation loss: 0.387642\n",
      "Epoch: 24 | Training loss: 0.120645 | Validation loss: 0.386029\n",
      "Epoch: 25 | Training loss: 0.117491 | Validation loss: 0.384634\n",
      "Epoch: 26 | Training loss: 0.114470 | Validation loss: 0.383385\n",
      "Epoch: 27 | Training loss: 0.111655 | Validation loss: 0.382273\n",
      "Epoch: 28 | Training loss: 0.109004 | Validation loss: 0.381111\n",
      "Epoch: 29 | Training loss: 0.106501 | Validation loss: 0.380074\n",
      "Epoch: 30 | Training loss: 0.104167 | Validation loss: 0.379176\n",
      "Epoch: 31 | Training loss: 0.101940 | Validation loss: 0.378252\n",
      "Epoch: 32 | Training loss: 0.099842 | Validation loss: 0.377455\n",
      "Epoch: 33 | Training loss: 0.097858 | Validation loss: 0.376728\n",
      "Epoch: 34 | Training loss: 0.095970 | Validation loss: 0.376028\n",
      "Epoch: 35 | Training loss: 0.094180 | Validation loss: 0.375525\n",
      "Epoch: 36 | Training loss: 0.092490 | Validation loss: 0.374897\n",
      "Epoch: 37 | Training loss: 0.090878 | Validation loss: 0.374288\n",
      "Epoch: 38 | Training loss: 0.089342 | Validation loss: 0.373773\n",
      "Epoch: 39 | Training loss: 0.087877 | Validation loss: 0.373345\n",
      "Epoch: 40 | Training loss: 0.086478 | Validation loss: 0.372901\n",
      "Epoch: 41 | Training loss: 0.085150 | Validation loss: 0.372512\n",
      "Epoch: 42 | Training loss: 0.083870 | Validation loss: 0.372182\n",
      "Epoch: 43 | Training loss: 0.082665 | Validation loss: 0.371805\n",
      "Epoch: 44 | Training loss: 0.081505 | Validation loss: 0.371478\n",
      "Epoch: 45 | Training loss: 0.080388 | Validation loss: 0.371239\n",
      "Epoch: 46 | Training loss: 0.079328 | Validation loss: 0.371027\n",
      "Epoch: 47 | Training loss: 0.078308 | Validation loss: 0.370801\n",
      "Epoch: 48 | Training loss: 0.077331 | Validation loss: 0.370531\n",
      "Epoch: 49 | Training loss: 0.076393 | Validation loss: 0.370327\n",
      "Epoch: 50 | Training loss: 0.075499 | Validation loss: 0.370141\n",
      "Epoch: 51 | Training loss: 0.074636 | Validation loss: 0.370017\n",
      "Epoch: 52 | Training loss: 0.073808 | Validation loss: 0.369914\n",
      "Epoch: 53 | Training loss: 0.073010 | Validation loss: 0.369768\n",
      "Epoch: 54 | Training loss: 0.072245 | Validation loss: 0.369650\n",
      "Epoch: 55 | Training loss: 0.071507 | Validation loss: 0.369610\n"
     ]
    }
   ],
   "source": [
    "w_tfidf, trl, devl = SGD(X_tr = X_tr_tfidf, Y_tr = train_label, \n",
    "                         X_dev = X_dev_tfidf, \n",
    "                         Y_dev = dev_label, \n",
    "                         lr=0.0001, \n",
    "                         alpha=0.01, \n",
    "                         epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "The model is overfit, because the final loss function value of the training data is much lower than that of the validation data.\n",
    "The reason for this result is over-training the training set data, resulting in local optimization, and the model's generalization ability is not good, so the error rate of the verification set is higher than the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:54.517668Z",
     "start_time": "2020-02-15T14:17:54.417118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,len(trl),len(trl))\n",
    "y1, y2 = trl, devl\n",
    " \n",
    "plt.plot(x, y1,label='Training loss')\n",
    "plt.plot(x, y2, label='Validation loss')\n",
    " \n",
    "plt.title('Training Monitoring')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:37:56.489814Z",
     "start_time": "2020-02-15T14:37:56.487014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8675\n",
      "Precision: 0.855072463768116\n",
      "Recall: 0.885\n",
      "F1-Score: 0.8697788697788698\n"
     ]
    }
   ],
   "source": [
    "p_class_tfidf = predict_class(X = X_test_tfidf, weights = w_tfidf)\n",
    "Y_te = test_label\n",
    "preds_te_tfidf = p_class_tfidf\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_tfidf))\n",
    "print('Precision:', precision_score(Y_te,preds_te_tfidf))\n",
    "print('Recall:', recall_score(Y_te,preds_te_tfidf))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print top-10 most positive and negative words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:38:17.845485Z",
     "start_time": "2020-02-15T14:38:17.842557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 positive words are ['great', 'hilarious', 'fun', 'overall', 'terrific', 'perfectly', 'definitely', 'memorable', 'simple', 'seen'] respectively.\n",
      "The top 10 negative words are ['bad', 'worst', 'boring', 'supposed', 'unfortunately', 'waste', 'awful', 'poor', 'script', 'nothing'] respectively.\n"
     ]
    }
   ],
   "source": [
    "dic_tfidf = {}\n",
    "for i,weight in enumerate(w_tfidf):\n",
    "    dic_tfidf[vocab[i]] = weight\n",
    "\n",
    "# get positive words\n",
    "dic_tfidf_p = {k:v for k, v in dic_tfidf.items() if v>=0}\n",
    "#order by weights\n",
    "dic_tfidf_p= dict(sorted(dic_tfidf_p.items(), key=lambda x:x[1], reverse=True))\n",
    "# select top 10 words\n",
    "dic_tfidf_p = {k: dic_tfidf_p[k] for k in list(dic_tfidf_p.keys())[:10]}\n",
    "\n",
    "# get negative words\n",
    "dic_tfidf_n = {k:v for k, v in dic_tfidf.items() if v<0}\n",
    "#order by weights\n",
    "dic_tfidf_n= dict(sorted(dic_tfidf_n.items(), key=lambda x:x[1], reverse=False))\n",
    "# select top 10 words\n",
    "dic_tfidf_n = {k: dic_tfidf_n[k] for k in list(dic_tfidf_n.keys())[:10]}\n",
    "\n",
    "print(\"The top 10 positive words are %s respectively.\"% list(dic_tfidf_p.keys()))\n",
    "print(\"The top 10 negative words are %s respectively.\"% list(dic_tfidf_n.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "We could use the classifier into a different domain such laptop reviews or restaurant reviews, because there are many adjectives and adverbs that can indicate positive or negative categories, such as 'great', 'hilarious', 'perfectly', etc. for positive features, 'bad', 'boring', 'unfortunately', etc. for negative features, these features would generalise well, so it's suitable in new fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lr = 0.001000, alpha = 0.010000;  F1-Score: 0.8627450980392156\n",
      "When lr = 0.001000, alpha = 0.001000;  F1-Score: 0.8627450980392156\n",
      "When lr = 0.001000, alpha = 0.000100;  F1-Score: 0.8627450980392156\n",
      "When lr = 0.000100, alpha = 0.010000;  F1-Score: 0.8697788697788698\n",
      "When lr = 0.000100, alpha = 0.001000;  F1-Score: 0.8669950738916256\n",
      "When lr = 0.000100, alpha = 0.000100;  F1-Score: 0.8641975308641976\n",
      "When lr = 0.000010, alpha = 0.010000;  F1-Score: 0.8669950738916256\n",
      "When lr = 0.000010, alpha = 0.001000;  F1-Score: 0.8669950738916256\n",
      "When lr = 0.000010, alpha = 0.000100;  F1-Score: 0.8669950738916256\n"
     ]
    }
   ],
   "source": [
    "# choose model hyperparameters: learning rate and regularisation strength\n",
    "lr_hyper = [0.001,0.0001,0.00001]\n",
    "alpha_hyper = [0.01,0.001,0.0001]\n",
    "\n",
    "for lr in range(len(lr_hyper)):\n",
    "    for alpha in range(len(alpha_hyper)):\n",
    "        w_tfidf, trl, devl = SGD(X_tr = X_tr_tfidf, Y_tr = train_label, \n",
    "                                             X_dev=X_dev_tfidf, \n",
    "                                             Y_dev=dev_label, \n",
    "                                             lr=lr_hyper[lr],\n",
    "                                             alpha=alpha_hyper[alpha], \n",
    "                                             epochs=100)\n",
    "        preds = predict_class(X = X_test_tfidf, weights = w_tfidf)\n",
    "        # If test this, we need change `SGD` to comment out that print(add # in front of print, do not print loss), \n",
    "        #then we can just see the results of F1-Score for several hyperparameters.\n",
    "        print('When lr = %f, alpha = %f; '%(lr_hyper[lr],alpha_hyper[alpha]), 'F1-Score:', f1_score(Y_te = test_label, preds_te_count = preds))\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that when lr = 0.00001, the F1-Score value is the maximum, 0.8669, in this case, the value of alpha have no effect on the F1-Score value.\n",
    "\n",
    "In general, the learning rate(lr) has a greater impact on model training, while regularisation strength(alpha) has little effect.\n",
    "\n",
    "When training the model, if the learning rate is reduced, the gradient descent rate will be slower and the model's convergence rate will also be slower, so in general, the number of training epochs required should  be increased to achieve the purpose of sufficient model convergence.\n",
    "\n",
    "Regularization can always improve generalization and help prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  | 0.827 | 0.840 | 0.833 |\n",
    "| BOW-tfidf  | 0.855 | 0.885 |  0.869 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Logistic Regression \n",
    "\n",
    "Now you need to train a Multiclass Logistic Regression (MLR) Classifier by extending the Binary model you developed above. You will use the MLR model to perform topic classification on the AG news dataset consisting of three classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class 1: World\n",
    "- Class 2: Sports\n",
    "- Class 3: Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to follow the same process as in Task 1 for data processing and feature extraction by reusing the functions you wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:03.212229Z",
     "start_time": "2020-02-15T14:18:03.185261Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the training, development and test sets\n",
    "data_tr = pd.read_csv('data_topic/train.csv',header=None, names=['label','text'])\n",
    "data_dev = pd.read_csv('data_topic/dev.csv',header=None, names=['label','text'])\n",
    "data_test = pd.read_csv('data_topic/test.csv',header=None, names=['label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:03.515585Z",
     "start_time": "2020-02-15T14:18:03.508299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1      1  Reuters - South Korean police used water canno...\n",
       "2      1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3      1  AFP - Sporadic gunfire and shelling took place...\n",
       "4      1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the raw texts into Python lists\n",
    "train_text = [x.lower() for x in data_tr['text'].tolist()] \n",
    "dev_text = [x.lower() for x in data_dev['text'].tolist()] \n",
    "test_text = [x.lower() for x in data_test['text'].tolist()]\n",
    "\n",
    "# put their corresponding labels into NumPy arrays\n",
    "train_label = np.array(data_tr['label'])\n",
    "dev_label = np.array(data_dev['label'])\n",
    "test_label = np.array(data_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  N-gram extraction from the above document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x_mul, tr_text_ngrams_mul = extract_ngrams(x_raw = train_text, stop_words=stop_words)\n",
    "dev_x_mul, dev_text_ngrams_mul = extract_ngrams(x_raw = dev_text, stop_words=stop_words)\n",
    "test_x_mul, test_text_ngrams_mul = extract_ngrams(x_raw = test_text, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_mul, tr_df_mul, tr_ngram_counts_mul = get_vocab(X_raw=train_text, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_count_mul = vectorise(X_ngram=tr_text_ngrams_mul, vocab=vocab_mul)\n",
    "X_dev_count_mul = vectorise(X_ngram=dev_text_ngrams_mul, vocab=vocab_mul)\n",
    "X_test_count_mul = vectorise(X_ngram=test_text_ngrams_mul, vocab=vocab_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [2., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_count_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 5000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_count_mul.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5000)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_count_mul.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idf vocabulary dictiongnary for training data\n",
    "vocab_idfs_tr_mul={}\n",
    "for word in vocab_mul:\n",
    "    doc_count=0\n",
    "    for text in tr_text_ngrams_mul:\n",
    "        if word in text:\n",
    "            doc_count+=1\n",
    "    vocab_idfs_tr_mul[word]=np.log((len(tr_text_ngrams_mul)/doc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idf vocabulary dictiongnary for development data\n",
    "vocab_idfs_dev_mul={}\n",
    "for word in vocab_mul:\n",
    "    doc_count=0\n",
    "    for text in dev_text_ngrams_mul:\n",
    "        if word in text:\n",
    "            doc_count+=1\n",
    "    vocab_idfs_dev_mul[word]=np.log((len(dev_text_ngrams_mul)+1) / (doc_count+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idf vocabulary dictiongnary for test data\n",
    "vocab_idfs_test_mul={}\n",
    "for word in vocab_mul:\n",
    "    doc_count=0\n",
    "    for text in test_text_ngrams_mul:\n",
    "        if word in text:\n",
    "            doc_count+=1\n",
    "    vocab_idfs_test_mul[word]=np.log((len(test_text_ngrams_mul)+1) / (doc_count+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training set, extract tf.idf vectors\n",
    "tfidf_mat=[]\n",
    "for i, word in enumerate(vocab_idfs_tr_mul.keys()):\n",
    "    value = X_tr_count_mul[:,i]\n",
    "    tfidf = value * vocab_idfs_tr_mul[word]\n",
    "    tfidf_mat.append(tfidf)\n",
    "\n",
    "X_tr_tfidf_mul = np.asarray(tfidf_mat)\n",
    "X_tr_tfidf_mul = np.transpose(X_tr_tfidf_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.33591815, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.33591815, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.33591815, 1.71479843, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [2.67183631, 1.71479843, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tfidf_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development set, extract tf.idf vectors\n",
    "tfidf_mat=[]\n",
    "for i, word in enumerate(vocab_idfs_dev_mul.keys()):\n",
    "    value = X_dev_count_mul[:,i]\n",
    "    tfidf = value * vocab_idfs_dev_mul[word]\n",
    "    tfidf_mat.append(tfidf)\n",
    "\n",
    "X_dev_tfidf_mul = np.asarray(tfidf_mat)\n",
    "X_dev_tfidf_mul = np.transpose(X_dev_tfidf_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test set, extract tf.idf vectors\n",
    "tfidf_mat=[]\n",
    "for i, word in enumerate(vocab_idfs_test_mul.keys()):\n",
    "    value = X_test_count_mul[:,i]\n",
    "    tfidf = value * vocab_idfs_test_mul[word]\n",
    "    tfidf_mat.append(tfidf)\n",
    "\n",
    "X_test_tfidf_mul = np.asarray(tfidf_mat)\n",
    "X_test_tfidf_mul = np.transpose(X_test_tfidf_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to change `SGD` to support multiclass datasets. First you need to develop a `softmax` function. It takes as input:\n",
    "\n",
    "- `z`: array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `smax`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    x_exp = np.exp(z)\n",
    "    # If it is a one-dimensional array, then axis = 0, otherwise, axis = 1.\n",
    "    if len(z.shape) >1:\n",
    "        x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    else:\n",
    "        x_sum = np.sum(x_exp, axis = 0)\n",
    "    smax = x_exp / x_sum    \n",
    "    return smax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then modify `predict_proba` and `predict_class` functions for the multiclass case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.445451Z",
     "start_time": "2020-02-15T14:18:07.442851Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    dot_p = np.dot(X,weights.T)\n",
    "    preds_proba = softmax(dot_p)\n",
    "    \n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.449814Z",
     "start_time": "2020-02-15T14:18:07.447145Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    proba = predict_proba(X = X, weights = weights)\n",
    "    if len(proba.shape) >1:\n",
    "        label_list = []\n",
    "        for row in proba:\n",
    "            row[row == max(row)] = 1\n",
    "            for i,value in enumerate(row):\n",
    "                if value == 1:\n",
    "                    label = i+1\n",
    "                    label_list.append(label)\n",
    "                    break;\n",
    "        preds_class = np.asarray(label_list)\n",
    "    else:\n",
    "        proba[proba == max(proba)] = 1\n",
    "        for i,value in enumerate(proba):\n",
    "            if value == 1:\n",
    "                preds_class = i+1\n",
    "                break;\n",
    "    \n",
    "    return preds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example and expected functionality of the functions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:08.059902Z",
     "start_time": "2020-02-15T14:18:08.056774Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.1,0.2],[0.2,0.1],[0.1,-0.2]])\n",
    "w = np.array([[2,-5],[-5,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:08.495464Z",
     "start_time": "2020-02-15T14:18:08.491074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33181223, 0.66818777],\n",
       "       [0.66818777, 0.33181223],\n",
       "       [0.89090318, 0.10909682]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:08.714215Z",
     "start_time": "2020-02-15T14:18:08.710098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to compute the categorical cross entropy loss (extending the binary loss to support multiple classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:30:48.047338Z",
     "start_time": "2020-02-15T14:30:48.044395Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_loss(X, Y, weights, num_classes=3, alpha=0.00001):\n",
    "    \n",
    "    y_pred = predict_proba(X = X, weights = weights)\n",
    "    #Clip y_pred between alpha and 1-alpha\n",
    "    y_pred = np.clip(y_pred, alpha, 1-alpha)\n",
    "    # If there are more than one document for input X\n",
    "    if len(X.shape) > 1:\n",
    "        loss = -np.log(y_pred[range(len(Y)),Y-1])\n",
    "    else:\n",
    "        loss = -np.log(y_pred[Y-1])\n",
    "    # Use L2 regularisation\n",
    "    L2_regularization = (alpha/2)*(np.sum(np.square(weights)))\n",
    "    l = loss + L2_regularization\n",
    "    \n",
    "    return l\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert vector to onehot form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(X,class_nums):\n",
    "    \n",
    "    array = np.eye(class_nums+1)[X]\n",
    "    \n",
    "    return np.delete(array,0,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support the categorical cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:10.176885Z",
     "start_time": "2020-02-15T14:18:10.165021Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], num_classes=3, lr=0.01, alpha=0.00001, epochs=5, tolerance=0.0001, print_progress=True):\n",
    "    \n",
    "    pre_loss_dev = 10.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    weights = np.zeros((3, X_tr.shape[1]))\n",
    "    #convert train set label Y_tr to onehot\n",
    "    Y_tr_onehot = onehot(X = Y_tr,class_nums = num_classes)\n",
    "    \n",
    "    #Perform multiple epochs over the training data \n",
    "    for i in range(epochs):\n",
    "        #Randomise the order of training data after each epoch\n",
    "        np.random.seed(i)\n",
    "        new_X_tr = np.random.permutation(X_tr)\n",
    "        np.random.seed(i)\n",
    "        new_Y_tr = np.random.permutation(Y_tr)\n",
    "        np.random.seed(i)\n",
    "        new_Y_tr_onehot = np.random.permutation(Y_tr_onehot)\n",
    "        tr_loss_list = [] # The list store training loss for each document.\n",
    "        for j, X_tr_row in enumerate(new_X_tr):\n",
    "            tr_loss = categorical_loss(X = X_tr_row, Y=new_Y_tr[j], weights = weights, alpha=alpha)\n",
    "            tr_loss_list.append(tr_loss)\n",
    "            y_pred = predict_proba(X = X_tr_row, weights = weights)\n",
    "            error = y_pred - new_Y_tr_onehot[j]\n",
    "            #upgrade weights\n",
    "            weights_list = []\n",
    "            for n in range(num_classes):\n",
    "                weights_row = weights[n] - lr*X_tr_row*error[n]\n",
    "                weights_list.append(weights_row)\n",
    "            weights = np.asarray(weights_list)\n",
    "            \n",
    "        #get average training loss\n",
    "        aver_tr_loss = np.mean(tr_loss_list)\n",
    "        training_loss_history.append(aver_tr_loss)\n",
    "        \n",
    "        #compute validation set loss\n",
    "        valid_loss = categorical_loss(X = X_dev, Y=Y_dev, weights = weights, alpha=alpha)\n",
    "        valid_loss = sum(valid_loss)/len(valid_loss)\n",
    "        validation_loss_history.append(valid_loss)\n",
    "        \n",
    "        #After each epoch print the training and development loss\n",
    "        print('Epoch: %d' % i, '| Training loss: %f' % aver_tr_loss, '| Validation loss: %f' % valid_loss)\n",
    "        \n",
    "        #Stop training if the difference between the current and previous validation loss is smaller than tolerance\n",
    "        if (pre_loss_dev-valid_loss)<tolerance:\n",
    "            break\n",
    "        pre_loss_dev = valid_loss\n",
    "\n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate you MLR following the same steps as in Task 1 for both Count and tfidf features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.990924 | Validation loss: 0.978116\n",
      "Epoch: 1 | Training loss: 0.840919 | Validation loss: 0.896250\n",
      "Epoch: 2 | Training loss: 0.748140 | Validation loss: 0.834814\n",
      "Epoch: 3 | Training loss: 0.682860 | Validation loss: 0.786755\n",
      "Epoch: 4 | Training loss: 0.633647 | Validation loss: 0.748075\n",
      "Epoch: 5 | Training loss: 0.594811 | Validation loss: 0.716139\n",
      "Epoch: 6 | Training loss: 0.563061 | Validation loss: 0.689248\n",
      "Epoch: 7 | Training loss: 0.536481 | Validation loss: 0.666249\n",
      "Epoch: 8 | Training loss: 0.513770 | Validation loss: 0.646323\n",
      "Epoch: 9 | Training loss: 0.494028 | Validation loss: 0.628842\n",
      "Epoch: 10 | Training loss: 0.476689 | Validation loss: 0.613415\n",
      "Epoch: 11 | Training loss: 0.461262 | Validation loss: 0.599666\n",
      "Epoch: 12 | Training loss: 0.447425 | Validation loss: 0.587299\n",
      "Epoch: 13 | Training loss: 0.434920 | Validation loss: 0.576119\n",
      "Epoch: 14 | Training loss: 0.423530 | Validation loss: 0.565949\n",
      "Epoch: 15 | Training loss: 0.413118 | Validation loss: 0.556682\n",
      "Epoch: 16 | Training loss: 0.403533 | Validation loss: 0.548226\n",
      "Epoch: 17 | Training loss: 0.394681 | Validation loss: 0.540351\n",
      "Epoch: 18 | Training loss: 0.386464 | Validation loss: 0.533054\n",
      "Epoch: 19 | Training loss: 0.378835 | Validation loss: 0.526341\n",
      "Epoch: 20 | Training loss: 0.371688 | Validation loss: 0.520029\n",
      "Epoch: 21 | Training loss: 0.365016 | Validation loss: 0.514195\n",
      "Epoch: 22 | Training loss: 0.358761 | Validation loss: 0.508755\n",
      "Epoch: 23 | Training loss: 0.352867 | Validation loss: 0.503708\n",
      "Epoch: 24 | Training loss: 0.347303 | Validation loss: 0.498925\n",
      "Epoch: 25 | Training loss: 0.342044 | Validation loss: 0.494428\n",
      "Epoch: 26 | Training loss: 0.337080 | Validation loss: 0.490216\n",
      "Epoch: 27 | Training loss: 0.332363 | Validation loss: 0.486162\n",
      "Epoch: 28 | Training loss: 0.327900 | Validation loss: 0.482399\n",
      "Epoch: 29 | Training loss: 0.323642 | Validation loss: 0.478835\n",
      "Epoch: 30 | Training loss: 0.319590 | Validation loss: 0.475451\n",
      "Epoch: 31 | Training loss: 0.315726 | Validation loss: 0.472255\n",
      "Epoch: 32 | Training loss: 0.312040 | Validation loss: 0.469254\n",
      "Epoch: 33 | Training loss: 0.308501 | Validation loss: 0.466352\n",
      "Epoch: 34 | Training loss: 0.305133 | Validation loss: 0.463601\n",
      "Epoch: 35 | Training loss: 0.301905 | Validation loss: 0.460978\n",
      "Epoch: 36 | Training loss: 0.298801 | Validation loss: 0.458492\n",
      "Epoch: 37 | Training loss: 0.295830 | Validation loss: 0.456106\n",
      "Epoch: 38 | Training loss: 0.292975 | Validation loss: 0.453865\n",
      "Epoch: 39 | Training loss: 0.290225 | Validation loss: 0.451683\n",
      "Epoch: 40 | Training loss: 0.287581 | Validation loss: 0.449612\n",
      "Epoch: 41 | Training loss: 0.285041 | Validation loss: 0.447639\n",
      "Epoch: 42 | Training loss: 0.282583 | Validation loss: 0.445709\n",
      "Epoch: 43 | Training loss: 0.280231 | Validation loss: 0.443911\n",
      "Epoch: 44 | Training loss: 0.277946 | Validation loss: 0.442163\n",
      "Epoch: 45 | Training loss: 0.275751 | Validation loss: 0.440492\n",
      "Epoch: 46 | Training loss: 0.273623 | Validation loss: 0.438861\n",
      "Epoch: 47 | Training loss: 0.271578 | Validation loss: 0.437330\n",
      "Epoch: 48 | Training loss: 0.269584 | Validation loss: 0.435844\n",
      "Epoch: 49 | Training loss: 0.267671 | Validation loss: 0.434437\n",
      "Epoch: 50 | Training loss: 0.265814 | Validation loss: 0.433056\n",
      "Epoch: 51 | Training loss: 0.264012 | Validation loss: 0.431741\n",
      "Epoch: 52 | Training loss: 0.262272 | Validation loss: 0.430476\n",
      "Epoch: 53 | Training loss: 0.260585 | Validation loss: 0.429255\n",
      "Epoch: 54 | Training loss: 0.258946 | Validation loss: 0.428074\n",
      "Epoch: 55 | Training loss: 0.257362 | Validation loss: 0.426960\n",
      "Epoch: 56 | Training loss: 0.255824 | Validation loss: 0.425889\n",
      "Epoch: 57 | Training loss: 0.254330 | Validation loss: 0.424849\n",
      "Epoch: 58 | Training loss: 0.252879 | Validation loss: 0.423849\n",
      "Epoch: 59 | Training loss: 0.251471 | Validation loss: 0.422869\n",
      "Epoch: 60 | Training loss: 0.250099 | Validation loss: 0.421919\n",
      "Epoch: 61 | Training loss: 0.248777 | Validation loss: 0.421029\n",
      "Epoch: 62 | Training loss: 0.247482 | Validation loss: 0.420182\n",
      "Epoch: 63 | Training loss: 0.246227 | Validation loss: 0.419350\n",
      "Epoch: 64 | Training loss: 0.245010 | Validation loss: 0.418553\n",
      "Epoch: 65 | Training loss: 0.243813 | Validation loss: 0.417763\n",
      "Epoch: 66 | Training loss: 0.242664 | Validation loss: 0.417030\n",
      "Epoch: 67 | Training loss: 0.241535 | Validation loss: 0.416305\n",
      "Epoch: 68 | Training loss: 0.240439 | Validation loss: 0.415620\n",
      "Epoch: 69 | Training loss: 0.239372 | Validation loss: 0.414949\n",
      "Epoch: 70 | Training loss: 0.238335 | Validation loss: 0.414291\n",
      "Epoch: 71 | Training loss: 0.237331 | Validation loss: 0.413670\n",
      "Epoch: 72 | Training loss: 0.236340 | Validation loss: 0.413076\n",
      "Epoch: 73 | Training loss: 0.235385 | Validation loss: 0.412508\n",
      "Epoch: 74 | Training loss: 0.234450 | Validation loss: 0.411939\n",
      "Epoch: 75 | Training loss: 0.233534 | Validation loss: 0.411388\n",
      "Epoch: 76 | Training loss: 0.232652 | Validation loss: 0.410879\n",
      "Epoch: 77 | Training loss: 0.231787 | Validation loss: 0.410381\n",
      "Epoch: 78 | Training loss: 0.230941 | Validation loss: 0.409896\n",
      "Epoch: 79 | Training loss: 0.230113 | Validation loss: 0.409418\n",
      "Epoch: 80 | Training loss: 0.229310 | Validation loss: 0.408941\n",
      "Epoch: 81 | Training loss: 0.228533 | Validation loss: 0.408510\n",
      "Epoch: 82 | Training loss: 0.227769 | Validation loss: 0.408099\n",
      "Epoch: 83 | Training loss: 0.227020 | Validation loss: 0.407706\n",
      "Epoch: 84 | Training loss: 0.226296 | Validation loss: 0.407343\n",
      "Epoch: 85 | Training loss: 0.225584 | Validation loss: 0.406968\n",
      "Epoch: 86 | Training loss: 0.224891 | Validation loss: 0.406605\n",
      "Epoch: 87 | Training loss: 0.224221 | Validation loss: 0.406265\n",
      "Epoch: 88 | Training loss: 0.223560 | Validation loss: 0.405940\n",
      "Epoch: 89 | Training loss: 0.222914 | Validation loss: 0.405610\n",
      "Epoch: 90 | Training loss: 0.222283 | Validation loss: 0.405278\n",
      "Epoch: 91 | Training loss: 0.221676 | Validation loss: 0.404993\n",
      "Epoch: 92 | Training loss: 0.221079 | Validation loss: 0.404716\n",
      "Epoch: 93 | Training loss: 0.220492 | Validation loss: 0.404437\n",
      "Epoch: 94 | Training loss: 0.219923 | Validation loss: 0.404176\n",
      "Epoch: 95 | Training loss: 0.219362 | Validation loss: 0.403925\n",
      "Epoch: 96 | Training loss: 0.218816 | Validation loss: 0.403676\n",
      "Epoch: 97 | Training loss: 0.218291 | Validation loss: 0.403447\n",
      "Epoch: 98 | Training loss: 0.217771 | Validation loss: 0.403246\n",
      "Epoch: 99 | Training loss: 0.217266 | Validation loss: 0.403040\n",
      "Epoch: 100 | Training loss: 0.216772 | Validation loss: 0.402846\n",
      "Epoch: 101 | Training loss: 0.216288 | Validation loss: 0.402649\n",
      "Epoch: 102 | Training loss: 0.215812 | Validation loss: 0.402453\n",
      "Epoch: 103 | Training loss: 0.215353 | Validation loss: 0.402282\n",
      "Epoch: 104 | Training loss: 0.214904 | Validation loss: 0.402105\n",
      "Epoch: 105 | Training loss: 0.214469 | Validation loss: 0.401969\n",
      "Epoch: 106 | Training loss: 0.214036 | Validation loss: 0.401824\n",
      "Epoch: 107 | Training loss: 0.213620 | Validation loss: 0.401689\n",
      "Epoch: 108 | Training loss: 0.213209 | Validation loss: 0.401550\n",
      "Epoch: 109 | Training loss: 0.212809 | Validation loss: 0.401429\n",
      "Epoch: 110 | Training loss: 0.212417 | Validation loss: 0.401292\n",
      "Epoch: 111 | Training loss: 0.212040 | Validation loss: 0.401190\n",
      "Epoch: 112 | Training loss: 0.211668 | Validation loss: 0.401083\n",
      "Epoch: 113 | Training loss: 0.211307 | Validation loss: 0.400989\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr = X_tr_count_mul, Y_tr = train_label, \n",
    "                                             X_dev=X_dev_count_mul, \n",
    "                                             Y_dev=dev_label,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.001, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot training and validation process and explain if your model overfit, underfit or is about right:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "The model is overfit, because the final loss function value of the training data is much lower than that of the validation data. The reason for this result is over-training the training set data, resulting in local optimization, and the model's generalization ability is not good, so the error rate of the verification set is higher than the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:31:09.903453Z",
     "start_time": "2020-02-15T14:31:09.901360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,len(loss_tr_count),len(loss_tr_count))\n",
    "y1, y2 = loss_tr_count, dev_loss_count\n",
    " \n",
    "plt.plot(x, y1,label='Training loss')\n",
    "plt.plot(x, y2, label='Validation loss')\n",
    " \n",
    "plt.title('Training Monitoring')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8644444444444445\n",
      "Precision: 0.819078947368421\n",
      "Recall: 0.83\n",
      "F1-Score: 0.8245033112582781\n"
     ]
    }
   ],
   "source": [
    "p_class_count = predict_class(X = X_test_count_mul, weights = w_count)\n",
    "Y_te = test_label\n",
    "preds_te_count = p_class_count\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print the top-10 words for each class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:32:26.224693Z",
     "start_time": "2020-02-15T14:32:26.221886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 words for class 1 are ['afp', ('athens', 'greece'), 'najaf', 'murder', 'leader', 'monday', 'president', 'minister', 'said', 'troops'] respectively.\n",
      "The top 10 words for class 2 are ['athens', 'ap', 'team', ('athens', 'reuters'), 'season', 'olympic', 'game', 'coach', 'players', 'win'] respectively.\n",
      "The top 10 words for class 3 are ['company', 'business', 'inc', 'corp', 'market', 'oil', 'million', 'billion', 'based', 'bank'] respectively.\n"
     ]
    }
   ],
   "source": [
    "for i in range(w_count.shape[0]):\n",
    "    \n",
    "    dic_count = {}\n",
    "    for j,weight in enumerate(w_count[i]):\n",
    "        dic_count[vocab_mul[j]] = weight\n",
    "\n",
    "    #order by weights\n",
    "    dic_count_class= dict(sorted(dic_count.items(), key=lambda x:x[1], reverse=True))\n",
    "    # select top 10 words\n",
    "    dic_count_class = {k: dic_count_class[k] for k in list(dic_count_class.keys())[:10]}\n",
    "\n",
    "    print(\"The top 10 words for class %d are %s respectively.\"% (i+1, list(dic_count_class.keys())))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we were to apply the classifier we've learned into a different domain, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "We could not use the classifier into a different domain, because the extracted feature words are basically nouns, these features are only available in a certain field. For example, the words 'murder', 'leader', etc. only apply to International issues, not to the field of Sports. \n",
    "\n",
    "So I think these features would not generalise well. And maybe for some adjective or adverb features, the classifier could pick up as important in the new domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lr = 0.010000, alpha = 0.100000;  F1-Score: 0.8225538971807628\n",
      "When lr = 0.010000, alpha = 0.010000;  F1-Score: 0.83\n",
      "When lr = 0.010000, alpha = 0.001000;  F1-Score: 0.8250825082508251\n",
      "When lr = 0.001000, alpha = 0.100000;  F1-Score: 0.8047138047138047\n",
      "When lr = 0.001000, alpha = 0.010000;  F1-Score: 0.8305647840531561\n",
      "When lr = 0.001000, alpha = 0.001000;  F1-Score: 0.8239202657807309\n",
      "When lr = 0.000100, alpha = 0.100000;  F1-Score: 0.8109028960817717\n",
      "When lr = 0.000100, alpha = 0.010000;  F1-Score: 0.8113522537562604\n",
      "When lr = 0.000100, alpha = 0.001000;  F1-Score: 0.8113522537562604\n"
     ]
    }
   ],
   "source": [
    "# choose model hyperparameters: learning rate and regularisation strength\n",
    "lr_hyper = [0.01,0.001,0.0001]\n",
    "alpha_hyper = [0.1,0.01,0.001]\n",
    "\n",
    "for lr in range(len(lr_hyper)):\n",
    "    for alpha in range(len(alpha_hyper)):\n",
    "        w_count, loss_tr_count, dev_loss_count = SGD(X_tr = X_tr_count_mul, Y_tr = train_label, \n",
    "                                             X_dev=X_dev_count_mul, \n",
    "                                             Y_dev=dev_label, \n",
    "                                             lr=lr_hyper[lr],\n",
    "                                             alpha=alpha_hyper[alpha], \n",
    "                                             epochs=100)\n",
    "        preds = predict_class(X = X_test_count_mul, weights = w_count)\n",
    "        # If test this, we need change `SGD` to comment out that print(add # in front of print, do not print loss), \n",
    "        #then we can just see the results of F1-Score for several hyperparameters.\n",
    "        print('When lr = %f, alpha = %f; '%(lr_hyper[lr],alpha_hyper[alpha]), 'F1-Score:', f1_score(Y_te = test_label, preds_te_count = preds))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that when lr = 0.001 and alpha = 0.01, the F1-Score value reaches the maximum, 0.83. As lr or alpha continues to increase, the F1-Score value becomes smaller. \n",
    "\n",
    "In general, the learning rate(lr) has a greater impact on model training, and regularisation strength(alpha) also has a certain degree of effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now evaluate BOW-tfidf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.958627 | Validation loss: 0.956724\n",
      "Epoch: 1 | Training loss: 0.760124 | Validation loss: 0.867729\n",
      "Epoch: 2 | Training loss: 0.646550 | Validation loss: 0.804482\n",
      "Epoch: 3 | Training loss: 0.571555 | Validation loss: 0.756887\n",
      "Epoch: 4 | Training loss: 0.517588 | Validation loss: 0.719420\n",
      "Epoch: 5 | Training loss: 0.476471 | Validation loss: 0.689058\n",
      "Epoch: 6 | Training loss: 0.443746 | Validation loss: 0.663801\n",
      "Epoch: 7 | Training loss: 0.416990 | Validation loss: 0.642392\n",
      "Epoch: 8 | Training loss: 0.394506 | Validation loss: 0.623917\n",
      "Epoch: 9 | Training loss: 0.375315 | Validation loss: 0.607820\n",
      "Epoch: 10 | Training loss: 0.358660 | Validation loss: 0.593657\n",
      "Epoch: 11 | Training loss: 0.344057 | Validation loss: 0.581029\n",
      "Epoch: 12 | Training loss: 0.331089 | Validation loss: 0.569718\n",
      "Epoch: 13 | Training loss: 0.319513 | Validation loss: 0.559480\n",
      "Epoch: 14 | Training loss: 0.309059 | Validation loss: 0.550186\n",
      "Epoch: 15 | Training loss: 0.299596 | Validation loss: 0.541733\n",
      "Epoch: 16 | Training loss: 0.290929 | Validation loss: 0.534015\n",
      "Epoch: 17 | Training loss: 0.283007 | Validation loss: 0.526798\n",
      "Epoch: 18 | Training loss: 0.275710 | Validation loss: 0.520115\n",
      "Epoch: 19 | Training loss: 0.268981 | Validation loss: 0.513981\n",
      "Epoch: 20 | Training loss: 0.262703 | Validation loss: 0.508210\n",
      "Epoch: 21 | Training loss: 0.256895 | Validation loss: 0.502851\n",
      "Epoch: 22 | Training loss: 0.251486 | Validation loss: 0.497846\n",
      "Epoch: 23 | Training loss: 0.246400 | Validation loss: 0.493205\n",
      "Epoch: 24 | Training loss: 0.241633 | Validation loss: 0.488788\n",
      "Epoch: 25 | Training loss: 0.237163 | Validation loss: 0.484647\n",
      "Epoch: 26 | Training loss: 0.232954 | Validation loss: 0.480740\n",
      "Epoch: 27 | Training loss: 0.228984 | Validation loss: 0.476994\n",
      "Epoch: 28 | Training loss: 0.225231 | Validation loss: 0.473497\n",
      "Epoch: 29 | Training loss: 0.221680 | Validation loss: 0.470181\n",
      "Epoch: 30 | Training loss: 0.218327 | Validation loss: 0.467017\n",
      "Epoch: 31 | Training loss: 0.215128 | Validation loss: 0.464032\n",
      "Epoch: 32 | Training loss: 0.212104 | Validation loss: 0.461187\n",
      "Epoch: 33 | Training loss: 0.209196 | Validation loss: 0.458470\n",
      "Epoch: 34 | Training loss: 0.206452 | Validation loss: 0.455844\n",
      "Epoch: 35 | Training loss: 0.203845 | Validation loss: 0.453379\n",
      "Epoch: 36 | Training loss: 0.201331 | Validation loss: 0.451032\n",
      "Epoch: 37 | Training loss: 0.198951 | Validation loss: 0.448773\n",
      "Epoch: 38 | Training loss: 0.196664 | Validation loss: 0.446625\n",
      "Epoch: 39 | Training loss: 0.194468 | Validation loss: 0.444537\n",
      "Epoch: 40 | Training loss: 0.192373 | Validation loss: 0.442560\n",
      "Epoch: 41 | Training loss: 0.190368 | Validation loss: 0.440670\n",
      "Epoch: 42 | Training loss: 0.188425 | Validation loss: 0.438841\n",
      "Epoch: 43 | Training loss: 0.186594 | Validation loss: 0.437086\n",
      "Epoch: 44 | Training loss: 0.184808 | Validation loss: 0.435390\n",
      "Epoch: 45 | Training loss: 0.183107 | Validation loss: 0.433769\n",
      "Epoch: 46 | Training loss: 0.181463 | Validation loss: 0.432201\n",
      "Epoch: 47 | Training loss: 0.179893 | Validation loss: 0.430732\n",
      "Epoch: 48 | Training loss: 0.178368 | Validation loss: 0.429291\n",
      "Epoch: 49 | Training loss: 0.176916 | Validation loss: 0.427901\n",
      "Epoch: 50 | Training loss: 0.175506 | Validation loss: 0.426562\n",
      "Epoch: 51 | Training loss: 0.174147 | Validation loss: 0.425269\n",
      "Epoch: 52 | Training loss: 0.172834 | Validation loss: 0.424020\n",
      "Epoch: 53 | Training loss: 0.171577 | Validation loss: 0.422816\n",
      "Epoch: 54 | Training loss: 0.170357 | Validation loss: 0.421653\n",
      "Epoch: 55 | Training loss: 0.169178 | Validation loss: 0.420559\n",
      "Epoch: 56 | Training loss: 0.168052 | Validation loss: 0.419494\n",
      "Epoch: 57 | Training loss: 0.166950 | Validation loss: 0.418459\n",
      "Epoch: 58 | Training loss: 0.165888 | Validation loss: 0.417465\n",
      "Epoch: 59 | Training loss: 0.164870 | Validation loss: 0.416478\n",
      "Epoch: 60 | Training loss: 0.163872 | Validation loss: 0.415513\n",
      "Epoch: 61 | Training loss: 0.162931 | Validation loss: 0.414610\n",
      "Epoch: 62 | Training loss: 0.161992 | Validation loss: 0.413742\n",
      "Epoch: 63 | Training loss: 0.161099 | Validation loss: 0.412893\n",
      "Epoch: 64 | Training loss: 0.160240 | Validation loss: 0.412065\n",
      "Epoch: 65 | Training loss: 0.159393 | Validation loss: 0.411263\n",
      "Epoch: 66 | Training loss: 0.158589 | Validation loss: 0.410492\n",
      "Epoch: 67 | Training loss: 0.157792 | Validation loss: 0.409755\n",
      "Epoch: 68 | Training loss: 0.157038 | Validation loss: 0.409035\n",
      "Epoch: 69 | Training loss: 0.156301 | Validation loss: 0.408327\n",
      "Epoch: 70 | Training loss: 0.155587 | Validation loss: 0.407652\n",
      "Epoch: 71 | Training loss: 0.154895 | Validation loss: 0.406995\n",
      "Epoch: 72 | Training loss: 0.154224 | Validation loss: 0.406363\n",
      "Epoch: 73 | Training loss: 0.153575 | Validation loss: 0.405760\n",
      "Epoch: 74 | Training loss: 0.152940 | Validation loss: 0.405146\n",
      "Epoch: 75 | Training loss: 0.152325 | Validation loss: 0.404555\n",
      "Epoch: 76 | Training loss: 0.151745 | Validation loss: 0.404008\n",
      "Epoch: 77 | Training loss: 0.151171 | Validation loss: 0.403474\n",
      "Epoch: 78 | Training loss: 0.150615 | Validation loss: 0.402953\n",
      "Epoch: 79 | Training loss: 0.150072 | Validation loss: 0.402431\n",
      "Epoch: 80 | Training loss: 0.149545 | Validation loss: 0.401919\n",
      "Epoch: 81 | Training loss: 0.149043 | Validation loss: 0.401436\n",
      "Epoch: 82 | Training loss: 0.148552 | Validation loss: 0.400981\n",
      "Epoch: 83 | Training loss: 0.148069 | Validation loss: 0.400528\n",
      "Epoch: 84 | Training loss: 0.147614 | Validation loss: 0.400112\n",
      "Epoch: 85 | Training loss: 0.147161 | Validation loss: 0.399690\n",
      "Epoch: 86 | Training loss: 0.146722 | Validation loss: 0.399278\n",
      "Epoch: 87 | Training loss: 0.146310 | Validation loss: 0.398891\n",
      "Epoch: 88 | Training loss: 0.145892 | Validation loss: 0.398507\n",
      "Epoch: 89 | Training loss: 0.145496 | Validation loss: 0.398128\n",
      "Epoch: 90 | Training loss: 0.145110 | Validation loss: 0.397750\n",
      "Epoch: 91 | Training loss: 0.144739 | Validation loss: 0.397400\n",
      "Epoch: 92 | Training loss: 0.144376 | Validation loss: 0.397061\n",
      "Epoch: 93 | Training loss: 0.144023 | Validation loss: 0.396725\n",
      "Epoch: 94 | Training loss: 0.143680 | Validation loss: 0.396398\n",
      "Epoch: 95 | Training loss: 0.143346 | Validation loss: 0.396093\n",
      "Epoch: 96 | Training loss: 0.143021 | Validation loss: 0.395774\n",
      "Epoch: 97 | Training loss: 0.142721 | Validation loss: 0.395477\n",
      "Epoch: 98 | Training loss: 0.142419 | Validation loss: 0.395201\n",
      "Epoch: 99 | Training loss: 0.142128 | Validation loss: 0.394931\n"
     ]
    }
   ],
   "source": [
    "w_tfidf, loss_tr_tfidf, dev_loss_tfidf = SGD(X_tr = X_tr_tfidf_mul, Y_tr = train_label, \n",
    "                                             X_dev=X_dev_tfidf_mul, \n",
    "                                             Y_dev=dev_label,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.0001, \n",
    "                                             alpha=0.01, \n",
    "                                             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot training and validation process and explain if your model overfit, underfit or is about right:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is overfit, because the final loss function value of the training data is much lower than that of the validation data. The reason for this result is over-training the training set data, resulting in local optimization, and the model's generalization ability is not good, so the error rate of the verification set is higher than the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,len(loss_tr_tfidf),len(loss_tr_tfidf))\n",
    "y1, y2 = loss_tr_tfidf, dev_loss_tfidf\n",
    " \n",
    "plt.plot(x, y1,label='Training loss')\n",
    "plt.plot(x, y2, label='Validation loss')\n",
    " \n",
    "plt.title('Training Monitoring')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8822222222222222\n",
      "Precision: 0.8471760797342193\n",
      "Recall: 0.85\n",
      "F1-Score: 0.848585690515807\n"
     ]
    }
   ],
   "source": [
    "p_class_tfidf = predict_class(X = X_test_tfidf_mul, weights = w_tfidf)\n",
    "Y_te = test_label\n",
    "preds_te_tfidf = p_class_tfidf\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_tfidf))\n",
    "print('Precision:', precision_score(Y_te,preds_te_tfidf))\n",
    "print('Recall:', recall_score(Y_te,preds_te_tfidf))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print the top-10 words for each class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 words for class 1 are ['afp', ('athens', 'greece'), 'said', 'murder', 'leader', 'beijing', 'najaf', 'president', 'monday', 'india'] respectively.\n",
      "The top 10 words for class 2 are ['athens', 'ap', 'team', 'olympic', ('athens', 'reuters'), 'season', 'game', 'games', 'coach', 'players'] respectively.\n",
      "The top 10 words for class 3 are ['company', 'business', 'market', 'oil', 'inc', 'corp', 'based', 'google', 'billion', 'million'] respectively.\n"
     ]
    }
   ],
   "source": [
    "for i in range(w_tfidf.shape[0]):\n",
    "    \n",
    "    dic_tfidf = {}\n",
    "    for j,weight in enumerate(w_tfidf[i]):\n",
    "        dic_tfidf[vocab_mul[j]] = weight\n",
    "\n",
    "    #order by weights\n",
    "    dic_tfidf_class= dict(sorted(dic_tfidf.items(), key=lambda x:x[1], reverse=True))\n",
    "    # select top 10 words\n",
    "    dic_tfidf_class = {k: dic_tfidf_class[k] for k in list(dic_tfidf_class.keys())[:10]}\n",
    "\n",
    "    print(\"The top 10 words for class %d are %s respectively.\"% (i+1, list(dic_tfidf_class.keys())))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we were to apply the classifier we've learned into a different domain, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "We could not use the classifier into a different domain, because the extracted feature words are basically nouns, these features are only available in a certain field. For example, the words 'murder', 'leader', etc. only apply to International issues, not to the field of Sports.\n",
    "\n",
    "So I think these features would not generalise well. And maybe for some adjective or adverb features, the classifier could pick up as important in the new domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lr = 0.010000, alpha = 0.100000;  F1-Score: 0.8189233278955953\n",
      "When lr = 0.010000, alpha = 0.010000;  F1-Score: 0.8189233278955953\n",
      "When lr = 0.010000, alpha = 0.001000;  F1-Score: 0.8110749185667753\n",
      "When lr = 0.001000, alpha = 0.100000;  F1-Score: 0.8504983388704319\n",
      "When lr = 0.001000, alpha = 0.010000;  F1-Score: 0.8452579034941764\n",
      "When lr = 0.001000, alpha = 0.001000;  F1-Score: 0.8333333333333334\n",
      "When lr = 0.000100, alpha = 0.100000;  F1-Score: 0.8504983388704319\n",
      "When lr = 0.000100, alpha = 0.010000;  F1-Score: 0.848585690515807\n",
      "When lr = 0.000100, alpha = 0.001000;  F1-Score: 0.848585690515807\n"
     ]
    }
   ],
   "source": [
    "# choose model hyperparameters: learning rate and regularisation strength\n",
    "lr_hyper = [0.01,0.001,0.0001]\n",
    "alpha_hyper = [0.1,0.01,0.001]\n",
    "\n",
    "for lr in range(len(lr_hyper)):\n",
    "    for alpha in range(len(alpha_hyper)):\n",
    "        w_tfidf, loss_tr_tfidf, dev_loss_tfidf = SGD(X_tr = X_tr_tfidf_mul, Y_tr = train_label, \n",
    "                                             X_dev=X_dev_tfidf_mul, \n",
    "                                             Y_dev=dev_label, \n",
    "                                             lr=lr_hyper[lr],\n",
    "                                             alpha=alpha_hyper[alpha], \n",
    "                                             epochs=100)\n",
    "        preds = predict_class(X = X_test_tfidf_mul, weights = w_tfidf)\n",
    "        # If test this, we need change `SGD` to comment out that print(add # in front of print, do not print loss), \n",
    "        #then we can just see the results of F1-Score for several hyperparameters.\n",
    "        print('When lr = %f, alpha = %f; '%(lr_hyper[lr],alpha_hyper[alpha]), 'F1-Score:', f1_score(Y_te = test_label, preds_te_count = preds))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that when lr = 0.001(or 0.0001) and alpha = 0.1, the F1-Score value reaches the maximum, 0.85. As alpha continues to increase, the F1-Score value becomes smaller. \n",
    "\n",
    "In general, the learning rate(lr) has a greater impact on model training, and regularisation strength(alpha) also has a certain degree of effect.\n",
    "\n",
    "When training the model, if the learning rate is reduced, the gradient descent rate will be slower and the model's convergence rate will also be slower, so in general, the number of training epochs required should be increased to achieve the purpose of sufficient model convergence.\n",
    "\n",
    "Regularization can always improve generalization and help prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:16:42.567569Z",
     "start_time": "2020-02-15T14:16:42.562560Z"
    }
   },
   "source": [
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  | 0.819 | 0.830 | 0.824 |\n",
    "| BOW-tfidf  | 0.847 | 0.850 | 0.848 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
